#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chinese Text Translator Tool
============================

Tool ƒë·ªÉ scan, tr√≠ch xu·∫•t v√† thay th·∫ø text ti·∫øng Trung trong file code.

Usage:
    python chinese_text_translator.py extract <file_path> [output_json]
    python chinese_text_translator.py apply <file_path> <translation_json> [output_file]
    python chinese_text_translator.py preview <file_path> <translation_json>

V√≠ d·ª•:
    # Tr√≠ch xu·∫•t text ti·∫øng Trung t·ª´ file
    python chinese_text_translator.py extract gui.py extracted_texts.json
    
    # Xem tr∆∞·ªõc k·∫øt qu·∫£ s·∫Ω √°p d·ª•ng
    python chinese_text_translator.py preview gui.py translations.json
    
    # √Åp d·ª•ng b·∫£n d·ªãch v√†o file
    python chinese_text_translator.py apply gui.py translations.json gui_translated.py
"""

import re
import json
import sys
import os
from typing import List, Dict, Tuple, Optional
import argparse
from dataclasses import dataclass, asdict


@dataclass
class ChineseText:
    """ƒê·ªëi t∆∞·ª£ng ch·ª©a th√¥ng tin v·ªÅ text ti·∫øng Trung t√¨m th·∫•y"""
    line_number: int
    original_text: str
    context: str  # D√≤ng code ch·ª©a text n√†y
    text_type: str  # 'string', 'comment', 'docstring'
    start_pos: int  # V·ªã tr√≠ b·∫Øt ƒë·∫ßu trong d√≤ng
    end_pos: int    # V·ªã tr√≠ k·∫øt th√∫c trong d√≤ng
    translation: str = ""  # B·∫£n d·ªãch (s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn sau)


class ChineseTextExtractor:
    """Class ch√≠nh ƒë·ªÉ tr√≠ch xu·∫•t v√† thay th·∫ø text ti·∫øng Trung"""
    
    # Pattern ƒë·ªÉ nh·∫≠n di·ªán k√Ω t·ª± ti·∫øng Trung (Unicode ranges)
    CHINESE_CHAR_PATTERN = r'[\u4e00-\u9fff\u3400-\u4dbf\u20000-\u2a6df\u2a700-\u2b73f\u2b740-\u2b81f\u2b820-\u2ceaf\uf900-\ufaff\u3300-\u33ff\ufe30-\ufe4f\uf900-\ufaff\u2f800-\u2fa1f]+'
    
    # Pattern ƒë·ªÉ t√¨m c√°c lo·∫°i text kh√°c nhau
    PATTERNS = {
        'single_quote_string': r"'([^'\\]*(\\.[^'\\]*)*)'",
        'double_quote_string': r'"([^"\\]*(\\.[^"\\]*)*)"',
        'triple_single_quote': r"'''(.*?)'''",
        'triple_double_quote': r'"""(.*?)"""',
        'single_line_comment': r'#\s*(.*?)(?:\n|$)',
        'f_string': r'f["\']([^"\'\\]*(\\.[^"\'\\]*)*)["\']',
    }
    
    def __init__(self):
        self.extracted_texts: List[ChineseText] = []
    
    def contains_chinese(self, text: str) -> bool:
        """Ki·ªÉm tra xem text c√≥ ch·ª©a k√Ω t·ª± ti·∫øng Trung kh√¥ng"""
        return bool(re.search(self.CHINESE_CHAR_PATTERN, text))
    
    def extract_from_file(self, file_path: str) -> List[ChineseText]:
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ text ti·∫øng Trung t·ª´ file"""
        self.extracted_texts = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except UnicodeDecodeError:
            # Th·ª≠ v·ªõi encoding kh√°c n·∫øu UTF-8 th·∫•t b·∫°i
            with open(file_path, 'r', encoding='gbk') as f:
                lines = f.readlines()
        
        for line_num, line in enumerate(lines, 1):
            self._extract_from_line(line, line_num)
        
        # S·∫Øp x·∫øp theo s·ªë d√≤ng
        self.extracted_texts.sort(key=lambda x: (x.line_number, x.start_pos))
        return self.extracted_texts
    
    def _extract_from_line(self, line: str, line_number: int):
        """Tr√≠ch xu·∫•t text ti·∫øng Trung t·ª´ m·ªôt d√≤ng"""
        original_line = line.rstrip('\n\r')
        
        # T√¨m trong docstring (triple quotes) tr∆∞·ªõc
        self._find_docstrings(line, line_number, original_line)
        
        # T√¨m trong comment
        self._find_comments(line, line_number, original_line)
        
        # T√¨m trong string literals
        self._find_strings(line, line_number, original_line)
    
    def _find_docstrings(self, line: str, line_number: int, original_line: str):
        """T√¨m text ti·∫øng Trung trong docstring"""
        for pattern_name in ['triple_double_quote', 'triple_single_quote']:
            pattern = self.PATTERNS[pattern_name]
            for match in re.finditer(pattern, line, re.DOTALL):
                content = match.group(1)
                if self.contains_chinese(content):
                    chinese_text = ChineseText(
                        line_number=line_number,
                        original_text=content.strip(),
                        context=original_line,
                        text_type='docstring',
                        start_pos=match.start(),
                        end_pos=match.end()
                    )
                    self.extracted_texts.append(chinese_text)
    
    def _find_comments(self, line: str, line_number: int, original_line: str):
        """T√¨m text ti·∫øng Trung trong comment"""
        pattern = self.PATTERNS['single_line_comment']
        for match in re.finditer(pattern, line):
            content = match.group(1).strip()
            if self.contains_chinese(content):
                chinese_text = ChineseText(
                    line_number=line_number,
                    original_text=content,
                    context=original_line,
                    text_type='comment',
                    start_pos=match.start(),
                    end_pos=match.end()
                )
                self.extracted_texts.append(chinese_text)
    
    def _find_strings(self, line: str, line_number: int, original_line: str):
        """T√¨m text ti·∫øng Trung trong string literals"""
        # Lo·∫°i b·ªè comment tr∆∞·ªõc khi t√¨m string ƒë·ªÉ tr√°nh false positive
        comment_pos = line.find('#')
        search_line = line[:comment_pos] if comment_pos != -1 else line
        
        for pattern_name in ['double_quote_string', 'single_quote_string', 'f_string']:
            pattern = self.PATTERNS[pattern_name]
            for match in re.finditer(pattern, search_line):
                content = match.group(1)
                if self.contains_chinese(content):
                    chinese_text = ChineseText(
                        line_number=line_number,
                        original_text=content,
                        context=original_line,
                        text_type='string',
                        start_pos=match.start(),
                        end_pos=match.end()
                    )
                    self.extracted_texts.append(chinese_text)
    
    def export_to_json(self, output_file: str):
        """Xu·∫•t danh s√°ch text ra file JSON ƒë·ªÉ d·ªãch"""
        export_data = {
            "metadata": {
                "total_texts": len(self.extracted_texts),
                "instruction": "ƒêi·ªÅn b·∫£n d·ªãch v√†o tr∆∞·ªùng 'translation' cho m·ªói text",
                "note": "Gi·ªØ nguy√™n original_text, ch·ªâ s·ª≠a tr∆∞·ªùng translation"
            },
            "texts": [asdict(text) for text in self.extracted_texts]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ƒê√£ xu·∫•t {len(self.extracted_texts)} text ti·∫øng Trung ra {output_file}")
        print(f"üìù H√£y m·ªü file JSON v√† ƒëi·ªÅn b·∫£n d·ªãch v√†o tr∆∞·ªùng 'translation'")
        
        # T·ª± ƒë·ªông t·∫°o file text-only v√† prompt
        self.export_text_only_and_prompt(output_file)
    
    def export_text_only_and_prompt(self, json_file: str):
        """Xu·∫•t file text-only v√† t·∫°o AI prompt"""
        base_name = os.path.splitext(json_file)[0]
        txt_file = f"{base_name}_texts_only.txt"
        prompt_file = f"{base_name}_ai_prompt.txt"
        
        # T·∫°o file ch·ªâ ch·ª©a text
        unique_texts = list(dict.fromkeys([text.original_text for text in self.extracted_texts]))
        
        with open(txt_file, 'w', encoding='utf-8') as f:
            for i, text in enumerate(unique_texts, 1):
                f.write(f"{i}. {text}\n")
        
        # T·∫°o AI prompt (ch·ªâ d·∫´n t·ªõi file, kh√¥ng embed text)
        prompt_content = f"""ü§ñ D·ªäCH TEXT TI·∫æNG TRUNG SANG TI·∫æNG VI·ªÜT

üìÑ TH√îNG TIN:
- T·ªïng s·ªë text c·∫ßn d·ªãch: {len(unique_texts)}
- File ch·ª©a text: {os.path.basename(txt_file)}
- File n√†y ch·ª©a danh s√°ch text ti·∫øng Trung ƒë∆∞·ª£c ƒë√°nh s·ªë t·ª´ 1-{len(unique_texts)}

üìã Y√äU C·∫¶U D·ªäCH:
- D·ªãch ch√≠nh x√°c, t·ª± nhi√™n sang ti·∫øng Vi·ªát
- Gi·ªØ nguy√™n format s·ªë th·ª© t·ª± (1. 2. 3...)
- V·ªõi text k·ªπ thu·∫≠t (programming): d·ªãch s√°t nghƒ©a, d·ªÖ hi·ªÉu
- V·ªõi text giao di·ªán: d·ªãch ng·∫Øn g·ªçn, th√¢n thi·ªán  
- Kh√¥ng th√™m b·ªõt n·ªôi dung

üîó C√ÅCH TH·ª∞C HI·ªÜN:
1. M·ªü file: {os.path.basename(txt_file)}
2. ƒê·ªçc to√†n b·ªô {len(unique_texts)} d√≤ng text ti·∫øng Trung
3. D·ªãch t·ª´ng d√≤ng sang ti·∫øng Vi·ªát

‚úÖ FORMAT TR·∫¢ V·ªÄ:
Ch·ªâ tr·∫£ v·ªÅ danh s√°ch ƒë√£ d·ªãch theo format:
1. [b·∫£n d·ªãch ti·∫øng Vi·ªát c·ªßa text 1]
2. [b·∫£n d·ªãch ti·∫øng Vi·ªát c·ªßa text 2]
3. [b·∫£n d·ªãch ti·∫øng Vi·ªát c·ªßa text 3]
...
{len(unique_texts)}. [b·∫£n d·ªãch ti·∫øng Vi·ªát c·ªßa text {len(unique_texts)}]

‚ö†Ô∏è L∆ØU √ù:
- Ch·ªâ tr·∫£ v·ªÅ b·∫£n d·ªãch, kh√¥ng gi·∫£i th√≠ch th√™m
- Gi·ªØ nguy√™n s·ªë th·ª© t·ª± nh∆∞ trong file g·ªëc
- ƒê·∫£m b·∫£o d·ªãch ƒë·ªß {len(unique_texts)} d√≤ng

C·∫£m ∆°n b·∫°n! üôè"""
        
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write(prompt_content)
        
        print(f"\nüìÑ BONUS: ƒê√£ t·∫°o th√™m:")
        print(f"   ‚Ä¢ {txt_file} - File ch·ªâ ch·ª©a text ƒë·ªÉ d·ªãch")
        print(f"   ‚Ä¢ {prompt_file} - AI prompt s·∫µn s√†ng copy-paste")
        print(f"\nü§ñ C√ÅCH S·ª¨ D·ª§NG:")
        print(f"   1. M·ªü {prompt_file}")
        print(f"   2. Copy to√†n b·ªô n·ªôi dung")
        print(f"   3. Paste v√†o ChatGPT/Claude/Gemini")
        print(f"   4. Nh·∫≠n k·∫øt qu·∫£ d·ªãch v√† apply v√†o tool")
    
    def apply_translations(self, file_path: str, translation_file: str, output_file: str = None):
        """√Åp d·ª•ng b·∫£n d·ªãch v√†o file g·ªëc"""
        # ƒê·ªçc file b·∫£n d·ªãch
        with open(translation_file, 'r', encoding='utf-8') as f:
            translation_data = json.load(f)
        
        translations = {
            text['original_text']: text['translation'] 
            for text in translation_data['texts'] 
            if text.get('translation', '').strip()
        }
        
        if not translations:
            print("‚ùå Kh√¥ng t√¨m th·∫•y b·∫£n d·ªãch n√†o trong file JSON!")
            return
        
        # ƒê·ªçc file g·ªëc
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk') as f:
                lines = f.readlines()
        
        # √Åp d·ª•ng b·∫£n d·ªãch
        modified_lines = []
        applied_count = 0
        
        for line_num, line in enumerate(lines, 1):
            modified_line = line
            
            # T√¨m v√† thay th·∫ø text trong d√≤ng n√†y
            for original_text, translation in translations.items():
                if original_text in line:
                    # Thay th·∫ø text ti·∫øng Trung b·∫±ng b·∫£n d·ªãch
                    modified_line = modified_line.replace(original_text, translation)
                    applied_count += 1
            
            modified_lines.append(modified_line)
        
        # Ghi file k·∫øt qu·∫£
        output_path = output_file or file_path.replace('.py', '_translated.py')
        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(modified_lines)
        
        print(f"‚úÖ ƒê√£ √°p d·ª•ng {applied_count} b·∫£n d·ªãch v√†o {output_path}")
    
    def preview_changes(self, file_path: str, translation_file: str):
        """Xem tr∆∞·ªõc c√°c thay ƒë·ªïi s·∫Ω ƒë∆∞·ª£c √°p d·ª•ng"""
        # ƒê·ªçc file b·∫£n d·ªãch
        with open(translation_file, 'r', encoding='utf-8') as f:
            translation_data = json.load(f)
        
        translations = {
            text['original_text']: text['translation'] 
            for text in translation_data['texts'] 
            if text.get('translation', '').strip()
        }
        
        if not translations:
            print("‚ùå Kh√¥ng t√¨m th·∫•y b·∫£n d·ªãch n√†o trong file JSON!")
            return
        
        print("üîç PREVIEW - C√°c thay ƒë·ªïi s·∫Ω ƒë∆∞·ª£c √°p d·ª•ng:")
        print("=" * 80)
        
        for i, (original, translation) in enumerate(translations.items(), 1):
            print(f"\n{i}. D√≤ng c√≥ text: '{original[:50]}...' n·∫øu d√†i")
            print(f"   G·ªëc:  {original}")
            print(f"   D·ªãch: {translation}")
        
        print(f"\nüìä T·ªïng c·ªông: {len(translations)} thay ƒë·ªïi s·∫Ω ƒë∆∞·ª£c √°p d·ª•ng")


def main():
    parser = argparse.ArgumentParser(description='Tool d·ªãch text ti·∫øng Trung trong code')
    subparsers = parser.add_subparsers(dest='command', help='L·ªánh th·ª±c hi·ªán')
    
    # L·ªánh extract
    extract_parser = subparsers.add_parser('extract', help='Tr√≠ch xu·∫•t text ti·∫øng Trung t·ª´ file')
    extract_parser.add_argument('file_path', help='ƒê∆∞·ªùng d·∫´n file c·∫ßn tr√≠ch xu·∫•t')
    extract_parser.add_argument('output_json', nargs='?', help='File JSON output (m·∫∑c ƒë·ªãnh: extracted_texts.json)')
    
    # L·ªánh apply
    apply_parser = subparsers.add_parser('apply', help='√Åp d·ª•ng b·∫£n d·ªãch v√†o file')
    apply_parser.add_argument('file_path', help='ƒê∆∞·ªùng d·∫´n file g·ªëc')
    apply_parser.add_argument('translation_json', help='File JSON ch·ª©a b·∫£n d·ªãch')
    apply_parser.add_argument('output_file', nargs='?', help='File output (m·∫∑c ƒë·ªãnh: <file>_translated.py)')
    
    # L·ªánh preview
    preview_parser = subparsers.add_parser('preview', help='Xem tr∆∞·ªõc thay ƒë·ªïi')
    preview_parser.add_argument('file_path', help='ƒê∆∞·ªùng d·∫´n file g·ªëc')
    preview_parser.add_argument('translation_json', help='File JSON ch·ª©a b·∫£n d·ªãch')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    extractor = ChineseTextExtractor()
    
    if args.command == 'extract':
        if not os.path.exists(args.file_path):
            print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {args.file_path}")
            return
        
        output_json = args.output_json or 'extracted_texts.json'
        
        print(f"üîç ƒêang scan file: {args.file_path}")
        texts = extractor.extract_from_file(args.file_path)
        
        if texts:
            extractor.export_to_json(output_json)
            print(f"\nüìã T√≥m t·∫Øt:")
            print(f"   - T·ªïng text t√¨m th·∫•y: {len(texts)}")
            
            # Th·ªëng k√™ theo lo·∫°i
            type_count = {}
            for text in texts:
                type_count[text.text_type] = type_count.get(text.text_type, 0) + 1
            
            for text_type, count in type_count.items():
                print(f"   - {text_type}: {count}")
        else:
            print("‚úÖ Kh√¥ng t√¨m th·∫•y text ti·∫øng Trung n√†o trong file")
    
    elif args.command == 'apply':
        if not os.path.exists(args.file_path):
            print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {args.file_path}")
            return
        
        if not os.path.exists(args.translation_json):
            print(f"‚ùå File b·∫£n d·ªãch kh√¥ng t·ªìn t·∫°i: {args.translation_json}")
            return
        
        print(f"üîÑ ƒêang √°p d·ª•ng b·∫£n d·ªãch t·ª´ {args.translation_json} v√†o {args.file_path}")
        extractor.apply_translations(args.file_path, args.translation_json, args.output_file)
    
    elif args.command == 'preview':
        if not os.path.exists(args.file_path):
            print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {args.file_path}")
            return
        
        if not os.path.exists(args.translation_json):
            print(f"‚ùå File b·∫£n d·ªãch kh√¥ng t·ªìn t·∫°i: {args.translation_json}")
            return
        
        extractor.preview_changes(args.file_path, args.translation_json)


if __name__ == '__main__':
    main()
