#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SangTacViet Final Crawler - Phi√™n b·∫£n t·ªëi ∆∞u ch·ªâ gi·ªØ code c·∫ßn thi·∫øt
Crawl truy·ªán t·ª´ URL search v·ªõi pagination
"""

import requests
import time
import sqlite3
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from datetime import datetime

class SangTacVietCrawler:
    def __init__(self, db_file='sangtacviet_final.db'):
        self.db_file = db_file
        self.base_url = 'https://sangtacviet.app'
        self.search_api = 'https://sangtacviet.app/io/searchtp/searchBooks'
        
        # T·∫°o session v·ªõi headers c·∫ßn thi·∫øt
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': 'https://sangtacviet.app',
        })
        
        # Set cookies c∆° b·∫£n
        cookies = {
            'hideavatar': 'false',
            'arouting': 'e',
            '_ga': 'GA1.1.1985677746.1758020680'
        }
        for name, value in cookies.items():
            self.session.cookies.set(name, value, domain='sangtacviet.app')
        
        self.init_db()
    
    def init_db(self):
        """T·∫°o database SQLite"""
        conn = sqlite3.connect(self.db_file)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS novels (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE,
                title TEXT,
                author TEXT,
                genres TEXT,
                status TEXT,
                description TEXT,
                chapters INTEGER DEFAULT 0,
                source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
        print(f"‚úÖ Database: {self.db_file}")
    
    def save_novel(self, novel_data):
        """L∆∞u truy·ªán v√†o database - N·∫øu tr√πng th√¨ ch·ªâ c·∫≠p nh·∫≠t s·ªë ch∆∞∆°ng"""
        conn = sqlite3.connect(self.db_file)
        try:
            url = novel_data.get('url', '')
            
            # Ki·ªÉm tra xem truy·ªán ƒë√£ t·ªìn t·∫°i ch∆∞a
            existing = conn.execute('SELECT chapters FROM novels WHERE url = ?', (url,)).fetchone()
            
            # Tr√≠ch xu·∫•t source t·ª´ URL
            source = 'unknown'
            if '/truyen/' in url:
                parts = url.split('/truyen/')[1].split('/')
                if parts:
                    source = parts[0]
            
            new_chapters = novel_data.get('chapters', 0)
            
            if existing:
                # Truy·ªán ƒë√£ t·ªìn t·∫°i - ch·ªâ c·∫≠p nh·∫≠t s·ªë ch∆∞∆°ng n·∫øu thay ƒë·ªïi
                old_chapters = existing[0]
                if new_chapters != old_chapters and new_chapters > 0:
                    conn.execute('''
                        UPDATE novels 
                        SET chapters = ?, updated_at = CURRENT_TIMESTAMP
                        WHERE url = ?
                    ''', (new_chapters, url))
                    conn.commit()
                    print(f"    üîÑ Updated chapters: {old_chapters} ‚Üí {new_chapters}")
                    return 'updated'
                else:
                    print(f"    ‚úÖ Exists (no change): {old_chapters} chapters")
                    return 'exists'
            else:
                # Truy·ªán m·ªõi - l∆∞u to√†n b·ªô th√¥ng tin
                conn.execute('''
                    INSERT INTO novels 
                    (url, title, author, genres, status, description, chapters, source)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    url,
                    novel_data.get('title', ''),
                    novel_data.get('author', ''),
                    novel_data.get('genres', ''),
                    novel_data.get('status', ''),
                    novel_data.get('description', ''),
                    new_chapters,
                    source
                ))
                conn.commit()
                return 'new'
                
        except Exception as e:
            print(f"‚ùå L·ªói DB: {e}")
            return False
        finally:
            conn.close()
    
    def get_novels_count(self):
        """ƒê·∫øm s·ªë truy·ªán trong database"""
        conn = sqlite3.connect(self.db_file)
        count = conn.execute('SELECT COUNT(*) FROM novels').fetchone()[0]
        conn.close()
        return count
    
    def parse_search_url(self, url):
        """Parse parameters t·ª´ URL search"""
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            
            # Convert list sang string
            result = {}
            for key, value_list in params.items():
                result[key] = value_list[0] if value_list else ''
            
            # Set default values
            defaults = {'find': '', 'minc': '500', 'sort': 'view', 'step': '3', 'tag': ''}
            for key, default_value in defaults.items():
                if key not in result:
                    result[key] = default_value
            
            print(f"üîó Parsed: minc={result['minc']}, sort={result['sort']}")
            return result
        except Exception as e:
            print(f"‚ùå Parse URL error: {e}")
            return None
    
    def search_novels(self, find='', minc='500', sort='view', step='3', tag='', page=1):
        """T√¨m ki·∫øm truy·ªán qua API"""
        try:
            params = {'find': find, 'minc': minc, 'sort': sort, 'step': step, 'tag': tag, 'p': str(page)}
            
            # Update referer
            referer_url = f"{self.base_url}/search/?{'&'.join([f'{k}={v}' for k, v in params.items() if v])}"
            self.session.headers.update({'Referer': referer_url})
            
            response = self.session.post(self.search_api, params=params, data='tabpage=', timeout=15)
            
            if response.status_code != 200:
                print(f"‚ùå API error: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            novel_links = soup.find_all('a', {'class': 'booksearch'})
            
            urls = []
            for link in novel_links:
                href = link.get('href')
                if href and '/truyen/' in href:
                    full_url = self.base_url + href if href.startswith('/') else href
                    if not full_url.endswith('/'):
                        full_url += '/'
                    urls.append(full_url)
            
            print(f"  üìö Found {len(urls)} novels on page {page}")
            return urls
        except Exception as e:
            print(f"‚ùå Search error: {e}")
            return []
    
    def crawl_novel_detail(self, url):
        """Crawl chi ti·∫øt m·ªôt truy·ªán"""
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            text = response.text
            novel = {'url': url}
            
            # Title
            h1_tag = soup.find('h1')
            title = h1_tag.get_text().strip() if h1_tag else ''
            if not title:
                title_tag = soup.find('title')
                if title_tag:
                    title = re.sub(r'\s*-\s*\d+\s*ch∆∞∆°ng.*$', '', title_tag.get_text().strip())
            novel['title'] = title or 'Kh√¥ng x√°c ƒë·ªãnh'
            
            # Author
            author_meta = soup.find('meta', {'property': 'og:novel:author'})
            author = author_meta.get('content', '').strip() if author_meta else ''
            if not author:
                author_match = re.search(r'T√°c gi·∫£:[\s\xa0]*([^<\n]+)', text, re.I)
                if author_match:
                    author = re.sub(r'&nbsp;|\s+', ' ', author_match.group(1)).strip()
            novel['author'] = author or 'Kh√¥ng x√°c ƒë·ªãnh'
            
            # Genres
            genre_match = re.search(r'Th·ªÉ lo·∫°i:[\s\xa0]*([^<\n]+)', text, re.I)
            genres = ''
            if genre_match:
                genres = re.sub(r'&nbsp;|\s+', ' ', genre_match.group(1)).strip()
                genres = re.sub(r'[,\s]+$', '', genres)
            novel['genres'] = genres or 'Kh√¥ng x√°c ƒë·ªãnh'
            
            # Description - crawl nhi·ªÅu ngu·ªìn ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß
            description = ''
            
            # 1. Th·ª≠ og:description meta tag
            desc_meta = soup.find('meta', {'property': 'og:description'})
            if desc_meta:
                description = desc_meta.get('content', '').strip()
            
            # 2. N·∫øu ch∆∞a c√≥ ho·∫∑c qu√° ng·∫Øn, th·ª≠ meta description
            if not description or len(description) < 100:
                desc_meta2 = soup.find('meta', {'name': 'description'})
                if desc_meta2:
                    desc_text = desc_meta2.get('content', '').strip()
                    if len(desc_text) > len(description) and not desc_text.startswith('ƒê·ªçc truy·ªán ch·ªØ'):
                        description = desc_text
            
            # 3. Th·ª≠ t√¨m trong HTML content - ph·∫ßn gi·ªõi thi·ªáu truy·ªán
            if not description or len(description) < 200:
                # T√¨m div book-sumary (ngu·ªìn ch√≠nh)
                book_summary = soup.find('div', {'id': 'book-sumary'})
                if book_summary:
                    # L·∫•y text t·ª´ span b√™n trong
                    span = book_summary.find('span', class_='textzoom')
                    if span:
                        # L·∫•y text v√† thay th·∫ø <br> b·∫±ng xu·ªëng d√≤ng
                        desc_html = str(span)
                        desc_html = desc_html.replace('<br>', '\n').replace('<br/>', '\n').replace('<br />', '\n')
                        # Parse l·∫°i ƒë·ªÉ l·∫•y text sach
                        desc_soup = BeautifulSoup(desc_html, 'html.parser')
                        description = desc_soup.get_text().strip()
                    else:
                        # N·∫øu kh√¥ng c√≥ span textzoom, l·∫•y to√†n b·ªô text trong div
                        description = book_summary.get_text().strip()
                
                # T√¨m c√°c selector kh√°c n·∫øu ch∆∞a c√≥
                if not description or len(description) < 200:
                    # T√¨m div book-summary (c√≥ th·ªÉ vi·∫øt kh√°c)
                    book_summary2 = soup.find('div', {'id': 'book-summary'})
                    if book_summary2:
                        description = book_summary2.get_text().strip()
                    
                    # T√¨m div c√≥ class ch·ª©a 'summary'
                    if not description or len(description) < 200:
                        summary_divs = soup.find_all('div', class_=lambda x: x and 'summary' in x.lower())
                        for div in summary_divs:
                            text = div.get_text().strip()
                            if len(text) > 200:
                                description = text
                                break
                
                # N·∫øu v·∫´n ch∆∞a c√≥, t√¨m c√°c div kh√°c
                if not description or len(description) < 100:
                    desc_divs = soup.find_all('div', class_=['book-intro', 'summary', 'description', 'intro'])
                    for div in desc_divs:
                        text = div.get_text().strip()
                        if len(text) > 100:
                            description = text
                            break
                
                # N·∫øu v·∫´n ch∆∞a c√≥, t√¨m trong c√°c p tag c√≥ n·ªôi dung d√†i
                if not description or len(description) < 100:
                    p_tags = soup.find_all('p')
                    for p in p_tags:
                        p_text = p.get_text().strip()
                        if (len(p_text) > 200 and 
                            not any(skip in p_text.lower() for skip in ['ch∆∞∆°ng', 't√°c gi·∫£', 'th·ªÉ lo·∫°i', 'ƒëƒÉng k√Ω', 'ƒë·ªçc truy·ªán']) and
                            p_text not in description):
                            if description:
                                description += '\n\n' + p_text
                            else:
                                description = p_text
                            if len(description) > 500:  # ƒê·ªß d√†i r·ªìi
                                break
            
            # Clean up description
            if description:
                # Lo·∫°i b·ªè c√°c k√Ω t·ª± th·ª´a
                description = re.sub(r'\s+', ' ', description)
                description = re.sub(r'&nbsp;|&amp;|&lt;|&gt;', ' ', description)
                description = description.strip()
                
                # Gi·ªõi h·∫°n ƒë·ªô d√†i t·ªëi ƒëa
                if len(description) > 2000:
                    description = description[:2000] + '...'
            
            novel['description'] = description or 'Kh√¥ng c√≥ m√¥ t·∫£'
            
            # Status
            status = 'Kh√¥ng x√°c ƒë·ªãnh'
            text_lower = text.lower()
            if 'ho√†n th√†nh' in text_lower:
                status = 'Ho√†n th√†nh'
            elif 'c√≤n ti·∫øp' in text_lower:
                status = 'C√≤n ti·∫øp'
            elif 'ƒëang ti·∫øn h√†nh' in text_lower:
                status = 'ƒêang ti·∫øn h√†nh'
            novel['status'] = status
            
            # Chapters
            chapter_match = re.search(r'(\d+)\s*ch∆∞∆°ng', text, re.I)
            chapters = int(chapter_match.group(1)) if chapter_match else 0
            novel['chapters'] = chapters
            
            return novel
        except Exception as e:
            print(f"‚ùå Crawl error: {e}")
            return None
    
    def crawl_from_url(self, search_url, max_pages=5, start_page=1):
        """Crawl t·ª´ URL search v·ªõi pagination"""
        params = self.parse_search_url(search_url)
        if not params:
            return 0
        
        print(f"\nüöÄ CRAWL SEARCH RESULTS")
        print(f"üìã Filters: minc={params['minc']}, sort={params['sort']}")
        print(f"üìÑ Pages: {start_page} ‚Üí {start_page + max_pages - 1} (total: {max_pages})")
        
        total_crawled = 0
        end_page = start_page + max_pages - 1
        
        for page in range(start_page, end_page + 1):
            print(f"\n--- TRANG {page}/{end_page} ---")
            
            novel_urls = self.search_novels(
                params['find'], params['minc'], params['sort'], 
                params['step'], params['tag'], page
            )
            
            if not novel_urls:
                print(f"‚ùå Kh√¥ng c√≥ truy·ªán ·ªü trang {page}")
                break
            
            # Crawl t·ª´ng truy·ªán
            page_crawled = 0
            page_new = 0
            page_updated = 0
            page_exists = 0
            
            for i, novel_url in enumerate(novel_urls, 1):
                print(f"  [{i:2d}/{len(novel_urls)}] Crawling...")
                
                novel_data = self.crawl_novel_detail(novel_url)
                if novel_data:
                    result = self.save_novel(novel_data)
                    if result:
                        page_crawled += 1
                        if result == 'new':
                            total_crawled += 1
                            page_new += 1
                        elif result == 'updated':
                            page_updated += 1
                        elif result == 'exists':
                            page_exists += 1
                        
                        print(f"       ‚úÖ {novel_data['title'][:50]}...")
                        print(f"          Author: {novel_data['author']}")
                        print(f"          Chapters: {novel_data['chapters']}")
                    else:
                        print(f"       ‚ùå DB Error")
                else:
                    print(f"       ‚ùå Crawl Failed")
                
                time.sleep(1.5)  # Delay gi·ªØa requests
            
            print(f"üìÑ Trang {page}: {page_crawled}/{len(novel_urls)} th√†nh c√¥ng")
            if page_new > 0:
                print(f"    üÜï {page_new} truy·ªán m·ªõi")
            if page_updated > 0:
                print(f"    üîÑ {page_updated} truy·ªán c·∫≠p nh·∫≠t ch∆∞∆°ng")
            if page_exists > 0:
                print(f"    ‚úÖ {page_exists} truy·ªán ƒë√£ t·ªìn t·∫°i")
            
            if page < end_page:
                print("‚è≥ Ch·ªù 3 gi√¢y...")
                time.sleep(3)
        
        print(f"\n‚úÖ HO√ÄN TH√ÄNH!")
        print(f"üìÜ K·∫øt qu·∫£ t·ªïng h·ª£p:")
        print(f"  üÜï {total_crawled} truy·ªán m·ªõi ƒë√£ th√™m v√†o DB")
        print(f"  üìÑ T·ªïng truy·ªán trong DB: {self.get_novels_count()}")
        
        return total_crawled

def main():
    print("=" * 50)
    print("    SANGTACVIET FINAL CRAWLER")
    print("=" * 50)
    
    crawler = SangTacVietCrawler()
    print(f"üìä Database hi·ªán t·∫°i: {crawler.get_novels_count()} truy·ªán\n")
    
    while True:
        print("-" * 40)
        print("1. Crawl t·ª´ URL search")
        print("2. Xem th·ªëng k√™")
        print("3. Xem description m·∫´u")
        print("0. Tho√°t")
        
        choice = input("\nCh·ªçn (0-3): ").strip()
        
        if choice == '0':
            print("üëã T·∫°m bi·ªát!")
            break
        
        elif choice == '1':
            print("\nüìã CRAWL T·ª™ URL SEARCH")
            print("V√≠ d·ª•: https://sangtacviet.app/search/?find=&minc=500&sort=view&tag=")
            
            url = input("\nNh·∫≠p URL search: ").strip()
            if not url:
                print("‚ùå Ch∆∞a nh·∫≠p URL!")
                continue
            
            # Nh·∫≠p s·ªë trang
            try:
                max_pages = int(input("S·ªë trang (m·∫∑c ƒë·ªãnh 5): ") or "5")
                max_pages = max(1, min(max_pages, 100))  # TƒÉng l√™n 100
            except:
                max_pages = 5
            
            # Nh·∫≠p trang b·∫Øt ƒë·∫ßu
            try:
                start_page = int(input("Trang b·∫Øt ƒë·∫ßu (m·∫∑c ƒë·ªãnh 1): ") or "1")
                start_page = max(1, start_page)
            except:
                start_page = 1
            
            end_page = start_page + max_pages - 1
            print(f"\nüìÑ S·∫Ω crawl: Trang {start_page} ‚Üí {end_page} (t·ªïng {max_pages} trang)")
            
            confirm = input(f"\nX√°c nh·∫≠n crawl? (y/n): ").strip().lower()
            if confirm == 'y':
                crawler.crawl_from_url(url, max_pages, start_page)
        
        elif choice == '2':
            count = crawler.get_novels_count()
            print(f"\nüìà TH·ªêNG K√ä: {count} truy·ªán")
            
            if count > 0:
                conn = sqlite3.connect(crawler.db_file)
                
                # Top sources
                sources = conn.execute('''
                    SELECT source, COUNT(*) as count 
                    FROM novels GROUP BY source 
                    ORDER BY count DESC LIMIT 5
                ''').fetchall()
                
                if sources:
                    print("\nüìö Top sources:")
                    for source, cnt in sources:
                        print(f"  ‚Ä¢ {source}: {cnt} truy·ªán")
                
                # Chapters stats
                stats = conn.execute('''
                    SELECT AVG(chapters) as avg, MAX(chapters) as max, MIN(chapters) as min
                    FROM novels WHERE chapters > 0
                ''').fetchone()
                
                if stats and stats[0]:
                    avg, max_ch, min_ch = stats
                    print(f"\nüìñ Ch∆∞∆°ng: TB={avg:.0f}, Max={max_ch}, Min={min_ch}")
                
                conn.close()
        
        elif choice == '3':
            count = crawler.get_novels_count()
            if count == 0:
                print("‚ùå Ch∆∞a c√≥ truy·ªán n√†o trong database!")
                continue
                
            conn = sqlite3.connect(crawler.db_file)
            
            # L·∫•y 3 truy·ªán c√≥ description d√†i nh·∫•t
            novels = conn.execute('''
                SELECT title, author, description, chapters 
                FROM novels 
                WHERE description != 'Kh√¥ng c√≥ m√¥ t·∫£' AND LENGTH(description) > 100
                ORDER BY LENGTH(description) DESC 
                LIMIT 3
            ''').fetchall()
            
            if novels:
                print(f"\nüìù M·∫™U DESCRIPTION (ƒê√ÄI NH·∫§T):")
                for i, (title, author, desc, chapters) in enumerate(novels, 1):
                    print(f"\n--- TRUY·ªÜN {i} ---")
                    print(f"üìö Ti√™u ƒë·ªÅ: {title}")
                    print(f"‚úçÔ∏è T√°c gi·∫£: {author}")
                    print(f"üìù Ch∆∞∆°ng: {chapters}")
                    print(f"üìù M√¥ t·∫£ ({len(desc)} k√Ω t·ª±):")
                    print("-" * 60)
                    print(desc)
                    print("-" * 60)
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y truy·ªán n√†o c√≥ description d√†i!")
            
            conn.close()
        
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")

if __name__ == "__main__":
    main()
